
-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-06-05 14:46:15.370]  Checkpoint path: /new_tts\logs-tacotron\model.ckpt
[2025-06-05 14:46:15.370]  Loading training data from: /new_tts\training/train.txt
[2025-06-05 14:46:15.371]  Using model: tacotron
[2025-06-05 14:46:15.371]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: transliteration_cleaners
  decay_learning_rate: True
  decoder_depth: 256
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 200
  min_level_db: -100
  num_freq: 1025
  num_mels: 80
  outputs_per_step: 5
  postnet_depth: 256
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  sample_rate: 20000
  use_cmudict: False

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-06-05 15:44:48.048]  Checkpoint path: logs-tacotron\model.ckpt
[2025-06-05 15:44:48.048]  Loading training data from: E:/nepali_tts/training/train.txt
[2025-06-05 15:44:48.048]  Using model: tacotron
[2025-06-05 15:44:48.048]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: transliteration_cleaners
  decay_learning_rate: True
  decoder_depth: 256
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 200
  min_level_db: -100
  num_freq: 1025
  num_mels: 80
  outputs_per_step: 5
  postnet_depth: 256
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  sample_rate: 20000
  use_cmudict: False

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-06-05 15:45:32.725]  Checkpoint path: logs-tacotron\model.ckpt
[2025-06-05 15:45:32.725]  Loading training data from: E:/new_tts/training/train.txt
[2025-06-05 15:45:32.725]  Using model: tacotron
[2025-06-05 15:45:32.725]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: transliteration_cleaners
  decay_learning_rate: True
  decoder_depth: 256
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 200
  min_level_db: -100
  num_freq: 1025
  num_mels: 80
  outputs_per_step: 5
  postnet_depth: 256
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  sample_rate: 20000
  use_cmudict: False
[2025-06-05 15:45:32.729]  Loaded metadata for 2064 examples (2.80 hours)
[2025-06-05 15:45:34.353]  Initialized Tacotron model. Dimensions: 
[2025-06-05 15:45:34.353]    embedding:               256
[2025-06-05 15:45:34.353]    prenet out:              128
[2025-06-05 15:45:34.353]    encoder out:             256
[2025-06-05 15:45:34.353]    attention out:           256
[2025-06-05 15:45:34.353]    concat attn & out:       512
[2025-06-05 15:45:34.353]    decoder cell out:        256
[2025-06-05 15:45:34.353]    decoder out (5 frames):  400
[2025-06-05 15:45:34.353]    decoder out (1 frame):   80
[2025-06-05 15:45:34.353]    postnet out:             256
[2025-06-05 15:45:34.354]    linear out:              1025
[2025-06-05 15:45:40.303]  Starting new training run at commit: None
[2025-06-05 15:45:59.908]  Generated 32 batches of size 32 in 19.602 sec
[2025-06-05 15:46:14.956]  Step 1       [34.643 sec/step, loss=0.89027, avg_loss=0.89027]
[2025-06-05 15:46:18.420]  Step 2       [19.051 sec/step, loss=0.86205, avg_loss=0.87616]

-----------------------------------------------------------------
Starting new training run
-----------------------------------------------------------------
[2025-06-05 15:47:20.269]  Checkpoint path: E:/new_tts\logs-tacotron\model.ckpt
[2025-06-05 15:47:20.269]  Loading training data from: E:/new_tts\training/train.txt
[2025-06-05 15:47:20.270]  Using model: tacotron
[2025-06-05 15:47:20.270]  Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 256
  batch_size: 32
  cleaners: transliteration_cleaners
  decay_learning_rate: True
  decoder_depth: 256
  embed_depth: 256
  encoder_depth: 256
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.002
  max_iters: 200
  min_level_db: -100
  num_freq: 1025
  num_mels: 80
  outputs_per_step: 5
  postnet_depth: 256
  power: 1.5
  preemphasis: 0.97
  prenet_depths: [256, 128]
  ref_level_db: 20
  sample_rate: 20000
  use_cmudict: False
[2025-06-05 15:47:20.276]  Loaded metadata for 2064 examples (2.80 hours)
[2025-06-05 15:47:21.883]  Initialized Tacotron model. Dimensions: 
[2025-06-05 15:47:21.883]    embedding:               256
[2025-06-05 15:47:21.884]    prenet out:              128
[2025-06-05 15:47:21.884]    encoder out:             256
[2025-06-05 15:47:21.884]    attention out:           256
[2025-06-05 15:47:21.884]    concat attn & out:       512
[2025-06-05 15:47:21.884]    decoder cell out:        256
[2025-06-05 15:47:21.884]    decoder out (5 frames):  400
[2025-06-05 15:47:21.884]    decoder out (1 frame):   80
[2025-06-05 15:47:21.884]    postnet out:             256
[2025-06-05 15:47:21.884]    linear out:              1025
[2025-06-05 15:47:27.786]  Starting new training run at commit: None
[2025-06-05 15:47:30.567]  Generated 32 batches of size 32 in 2.780 sec
[2025-06-05 15:47:35.810]  Step 1       [8.012 sec/step, loss=0.83332, avg_loss=0.83332]
[2025-06-05 15:47:38.565]  Step 2       [5.381 sec/step, loss=0.81447, avg_loss=0.82390]
[2025-06-05 15:47:48.476]  Step 3       [6.889 sec/step, loss=0.87421, avg_loss=0.84067]
[2025-06-05 15:47:54.231]  Step 4       [6.605 sec/step, loss=0.86420, avg_loss=0.84655]
